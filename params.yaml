model:
  naive:
    epochs: 1
    batch_size: 8
    embed_dim: 4
    num_heads: 4
    num_transformer_blocks: 16
    ff_dim: 32
  patch:
    epochs: 1
    batch_size: 8
    embed_dim: 4
    num_heads: 4
    num_transformer_blocks: 64
    ff_dim: 128
  cnn:
    epochs: 1
    batch_size: 32
    mlp_dim: 128
    filters:
      - 16
      - 32
      - 64
    kernel_size: 3
  experiment1:
    epochs: 5
    batch_size: 8
    embed_dim: 4
    num_heads: 4
    ff_dim: 128
  experiment2:
    epochs: 10
    batch_size: 8
    patch_size: 7
    cnn:
      20k:
        mlp_dim: 128
        filters:
          - 4
          - 8
          - 16
          - 64
        kernel_size: 3
      40k:
        mlp_dim: 128
        filters:
          - 8
          - 16
          - 32
        kernel_size: 3
      90k:
        mlp_dim: 128
        filters:
          - 16
          - 32
          - 64
        kernel_size: 3
      200k:
        mlp_dim: 128
        filters:
          - 16
          - 32
          - 64
          - 256
        kernel_size: 3
    patch:
      20k:
        embed_dim: 4
        num_heads: 4
        num_transformer_blocks: 16
        ff_dim: 128
      40k:
        embed_dim: 4
        num_heads: 4
        num_transformer_blocks: 32
        ff_dim: 128
      90k:
        embed_dim: 4
        num_heads: 4
        num_transformer_blocks: 64
        ff_dim: 128
      200k:
        embed_dim: 4
        num_heads: 4
        num_transformer_blocks: 128
        ff_dim: 128
  experiment3:
    epochs: 10
    batch_size: 8
    embed_dim: 4
    num_heads: 4
    ff_dim: 128
  experiment4:
    epochs: 2
    batch_size: 8
    learning_rate: 0.0001
    vit_types:
      - vit_b16
      - vit_b32
      - vit_l16
      - vit_l32
evaluate:
  attention_visualization:
    image_num: 32 # good: 9, 16 - middle: 3, 16 - bad: 32, 35

